{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Immaczx/SemanticSegmentation/blob/master/Notebooks/DAResUnetSegmentationZeaMaysSeeds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHINBlmO2MyY",
        "outputId": "15025d23-176b-45d7-d8ed-61aa9f10c165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gcpds-image-segmentation (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.image_segmentation.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Immaczx/SemanticSegmentation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc7rA8FWfACi",
        "outputId": "79f4ebef-a0e3-468f-fb23-1b43090f3892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemanticSegmentation'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (184/184), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 184 (delta 84), reused 144 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (184/184), 42.48 KiB | 3.54 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK_MWgkgkXiS"
      },
      "outputs": [],
      "source": [
        "from SemanticSegmentation import utils\n",
        "import os\n",
        "import math as m\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gcpds.image_segmentation.losses import DiceCoefficient\n",
        "from gcpds.image_segmentation.metrics import Jaccard, Sensitivity, Specificity\n",
        "from gcpds.image_segmentation.metrics import DiceCoefficientMetric\n",
        "from gcpds.image_segmentation.models import unet_baseline, fcn_baseline, segnet_baseline, res_unet_baseline\n",
        "from gcpds.image_segmentation.datasets.segmentation import ZeaMaysSeeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6m0WPcp45fu"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras import layers\n",
        "\n",
        "IMG_SIZE=256\n",
        "batch_size = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# Create a generator.\n",
        "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
        "\n",
        "def preprocess(img,mask):\n",
        "    img = tf.image.resize(img,(256,256))\n",
        "    mask = tf.image.resize(mask,(256,256))#Ch 1: Seed, Ch 2: No germinate, Ch 3: germinate\n",
        "    mask = tf.cast(mask>0, tf.float32)\n",
        "    mask = mask[...,0][..., None]\n",
        "    return img,mask\n",
        "\n",
        "class Augment(tf.keras.layers.Layer):\n",
        "  def __init__(self, seed=42):\n",
        "    super().__init__()\n",
        "    # both use the same seed, so they'll make the same random changes.\n",
        "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "\n",
        "  def call(self, inputs, labels):\n",
        "    inputs = self.augment_inputs(inputs)\n",
        "    labels = self.augment_labels(labels)\n",
        "    seed = rng.make_seeds(2)[0]\n",
        "    image, label = augment((inputs, labels), seed)\n",
        "    return inputs, labels\n",
        "\n",
        "def augment(image_label, seed):\n",
        "  image, label = image_label\n",
        "  # Make a new seed.\n",
        "  new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]\n",
        "  # Random brightness.\n",
        "  image = tf.image.stateless_random_brightness(\n",
        "      image, max_delta=0.5, seed=new_seed)\n",
        "  # Random contrast\n",
        "  image = tf.image.stateless_random_contrast(\n",
        "      image, lower=0.1, upper=0.9, seed=new_seed)\n",
        "  image = tf.clip_by_value(image, 0, 1)\n",
        "  return image, label\n",
        "# Create a wrapper function for updating seeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIQF9VPu3Yx5"
      },
      "outputs": [],
      "source": [
        "image_size=256\n",
        "out_channels = 1\n",
        "\n",
        "model = res_unet_baseline(input_shape=(image_size,image_size,3), out_channels=out_channels)\n",
        "model.compile(loss=DiceCoefficient(), optimizer='Adam', metrics=[DiceCoefficientMetric(), Jaccard(), Sensitivity(), Specificity()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA12H6cC7-YH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6379b7-7b48-404a-82c6-7cd70dfb9271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14cI9XDnl6TS6uWSpnC-ifoGy3iLe4u0l&confirm=t\n",
            "To: /usr/local/lib/python3.10/dist-packages/gcpds/image_segmentation/datasets/segmentation/Datasets/ZeaMaysSeeds/ZeaMaysSeeds.zip\n",
            "100%|██████████| 122M/122M [00:01<00:00, 96.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Number of images for Partition 1: 2304\n",
            " Number of images for Partition 2: 576\n",
            " Number of images for Partition 3: 320\n"
          ]
        }
      ],
      "source": [
        "num = np.random.randint(0, 100)\n",
        "dataset = ZeaMaysSeeds(split=[0.1,0.2], seed = num)\n",
        "train,val,test = dataset()\n",
        "\n",
        "train = train.map(lambda x,y,id:preprocess(x,y))\n",
        "train = train.batch(1)\n",
        "val = val.map(lambda x,y,id:preprocess(x,y))\n",
        "val = val.batch(1)\n",
        "test = test.map(lambda x,y,id:preprocess(x,y))\n",
        "test = test.batch(1)\n",
        "\n",
        "train = train.cache()\n",
        "val = val.cache()\n",
        "test = test.cache()\n",
        "\n",
        "train_ds = (\n",
        "  train\n",
        "  .shuffle(1000)\n",
        "  .map(Augment(), num_parallel_calls=AUTOTUNE)\n",
        "  .batch(batch_size)\n",
        "  .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "train_da = train.cache()\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "model_history = model.fit(train_da,validation_data=val, epochs=100, verbose=0)\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "fig = plt.figure(figsize=(16,9))\n",
        "\n",
        "gs = fig.add_gridspec(2, 2)\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "ax2 = fig.add_subplot(gs[1, :-1])\n",
        "ax3 = fig.add_subplot(gs[1:, -1])\n",
        "\n",
        "for i in model_history.history:\n",
        "  ax1.plot(model_history.history[i],label=i)\n",
        "ax1.set_title('Model history')\n",
        "ax1.legend()\n",
        "ax1.grid()\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "img_test = tf.zeros((0, 256, 256, 3), dtype=tf.float32)\n",
        "mask_test = tf.zeros((0, 256, 256,1), dtype=tf.float32)\n",
        "\n",
        "for img, mask in test:\n",
        "    img_test = tf.concat([img_test, img], axis=0)\n",
        "    mask_test = tf.concat([mask_test, mask], axis=0)\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "mask_pred = model.predict(img_test)\n",
        "mask_pred = np.where(mask_pred > .5, 1, 0)\n",
        "evaluate_history = model.evaluate(img_test, mask_test)\n",
        "bars = ax3.barh([\"loss\",\"Dice\",\"Jaccard\",\"Sensitivity\",\"Specificity\"],np.abs(evaluate_history))\n",
        "ax3.bar_label(bars)\n",
        "ax3.set_title('Evaluate metrics')\n",
        "ax3.grid()\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Then, convert the predicted and ground truth masks to single-dimensional arrays\n",
        "pred_flat = tf.reshape(mask_pred, [-1])\n",
        "true_flat = tf.reshape(mask_test, [-1])\n",
        "\n",
        "# Plot the confusion matrix\n",
        "utils.plot_confusion_matrix(true_flat, pred_flat, classes=['Background','Seed'], ax = ax2)\n",
        "ax2.set_title('confusion matrix')\n",
        "\n",
        "# Ajustar los espacios entre los subplots\n",
        "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "\n",
        "# Mostrar la figura\n",
        "plt.show()\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "del train, val, test, dataset\n",
        "tf.keras.backend.clear_session()\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzBMcj-eCxAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
